{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "斗罗大陆\n",
      "作者：唐家三少\n",
      "本书来自【无名txt小说下载网-www.wmtxt.com】站内精心编辑制作 提供免费下载多谢你的支持！本站所有书籍均来自网络收集，站内精心编辑制作 提供免费下载。版权属作者或出版社所有。  精彩全本TXT小说下载请登www.wmtxt.com\n",
      "第一集 斗罗世界  \n",
      "引子 穿越的唐家三少\n",
      "巴蜀，历来有天府之国的美誉，其中，最有名的门派莫过于唐门。\n",
      "唐门所在是一个神秘的地方，许多人只知道，那是一个半山腰，而唐门所在这座山的山顶有一个令人胆颤心惊的名字，——鬼见愁。\n",
      "从鬼见愁悬崖上扔出一块石头，要足足数上十九下才会听到石落山底的回声，可见其高，也正是因为这十九秒，尚超过十八层地狱一筹，故而得名。\n",
      "一名身穿灰衣的青年正站在鬼见愁顶峰，凛冽的山风不能令他的身体有丝毫移动，从他胸口处那斗大的唐字就可以认出，他来自唐门，灰衣代表的，是唐门外门弟子。\n",
      "他今年二十九岁，因出生不久就进入唐门，在外门弟子的辈分中排名第三，因此外门弟子称他一声三少。当然，到了内门弟子口中，就变成了唐三。\n",
      "唐门从建立时开始就分为内外两门，外门都是外姓或被授予唐姓的弟子，而内门，则是唐门直系所属，家族传承。\n",
      "此时，唐三脸上的表情很丰富，时而笑，时而哭，但无论如何，都无法掩盖他的那发自内心的兴奋。\n",
      "二十九年了，自从二十九年前他被外门长老唐蓝太爷在襁褓时就捡回唐门时开始，唐门就是他的家，而唐门的暗器就是他的一切。\n",
      "突然，唐三脸色骤然一变，但很快又释然了，有些苦涩的自言自语道：“该来的终究还是来了。”\n",
      "十七道身影，十七道白色的身影，宛如星丸跳跃一般从山腰处朝山顶方向而来，这十七道身影的主人，年纪最小的也超过了五旬，一个个神色凝重，他们身穿的白袍代表的是内门，而胸前那金色的唐字则是唐门长老的象征。\n",
      "唐门内门长老堂包括掌门唐大先生在内，一共有十七位长老，此时登山的，也正是十七位。就算是武林大会也不可能惊动唐门全部长老同时出动，要知道，这唐门长老之中，年纪最大的已经超过了两个甲子。\n",
      "这些唐门长老的修为，无一不是已臻化境，只是转眼的工夫，他们就已经来到了山顶。\n",
      "外门弟子见到内门长老，只有跪倒迎接的份，但此时，唐三却没有动，他只是静静的看着这些脸色凝重的长老来到自己面前，挡住了所有的去路，而在他背后，是鬼见愁。\n",
      "放下三朵佛怒唐莲，唐三投下最后那恋恋不舍的一眼，嘴角处流露出一丝欣慰的微笑，他毕竟成功\n",
      "3942 unique characters\n"
     ]
    }
   ],
   "source": [
    "#path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "path_to_file=\"斗罗大陆.txt\"\n",
    "text = open(path_to_file).read()\n",
    "print(text[:1000])\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "chunks = tf.data.Dataset.from_tensor_slices(text_as_int).batch(seq_length+1, drop_remainder=True)\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "dataset = chunks.map(split_input_target)\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1009152   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    multiple                  3935232   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  4040550   \n",
      "=================================================================\n",
      "Total params: 8,984,934\n",
      "Trainable params: 8,984,934\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class Model(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, units):\n",
    "    super(Model, self).__init__()\n",
    "    self.units = units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                     return_sequences=True,\n",
    "                                     recurrent_activation='sigmoid',\n",
    "                                     recurrent_initializer='glorot_uniform',\n",
    "                                     stateful=True)\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "  def call(self, x):\n",
    "    embedding = self.embedding(x)\n",
    "    output = self.gru(embedding)\n",
    "    prediction = self.fc(output)\n",
    "    return prediction\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "model = Model(vocab_size, embedding_dim, units)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "def loss_function(real, preds):\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels=real, logits=preds)\n",
    "model.build(tf.TensorShape([BATCH_SIZE, seq_length]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.8595\n",
      "Epoch 1 Batch 100 Loss 3.0639\n",
      "Epoch 1 Batch 200 Loss 2.9052\n",
      "Epoch 1 Batch 300 Loss 2.8013\n",
      "Epoch 1 Batch 400 Loss 2.6918\n",
      "Epoch 1 Loss 2.8856\n",
      "Time taken for 1 epoch 279.09106969833374 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.1986\n",
      "Epoch 2 Batch 100 Loss 2.5323\n",
      "Epoch 2 Batch 200 Loss 2.5210\n",
      "Epoch 2 Batch 300 Loss 2.4448\n",
      "Epoch 2 Batch 400 Loss 2.3975\n",
      "Epoch 2 Loss 2.5993\n",
      "Time taken for 1 epoch 278.71822237968445 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.9575\n",
      "Epoch 3 Batch 100 Loss 2.3244\n",
      "Epoch 3 Batch 200 Loss 2.3276\n",
      "Epoch 3 Batch 300 Loss 2.2708\n",
      "Epoch 3 Batch 400 Loss 2.2509\n",
      "Epoch 3 Loss 2.4248\n",
      "Time taken for 1 epoch 278.7239639759064 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.8292\n",
      "Epoch 4 Batch 100 Loss 2.2210\n",
      "Epoch 4 Batch 200 Loss 2.2110\n",
      "Epoch 4 Batch 300 Loss 2.1791\n",
      "Epoch 4 Batch 400 Loss 2.1606\n",
      "Epoch 4 Loss 2.3225\n",
      "Time taken for 1 epoch 278.7929754257202 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.7828\n",
      "Epoch 5 Batch 100 Loss 2.1333\n",
      "Epoch 5 Batch 200 Loss 2.1469\n",
      "Epoch 5 Batch 300 Loss 2.0983\n",
      "Epoch 5 Batch 400 Loss 2.0908\n",
      "Epoch 5 Loss 2.2578\n",
      "Time taken for 1 epoch 278.97735834121704 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.7494\n",
      "Epoch 6 Batch 100 Loss 2.0852\n",
      "Epoch 6 Batch 200 Loss 2.0812\n",
      "Epoch 6 Batch 300 Loss 2.0605\n",
      "Epoch 6 Batch 400 Loss 2.0422\n",
      "Epoch 6 Loss 2.1953\n",
      "Time taken for 1 epoch 278.8442192077637 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.7320\n",
      "Epoch 7 Batch 100 Loss 2.0411\n",
      "Epoch 7 Batch 200 Loss 2.0358\n",
      "Epoch 7 Batch 300 Loss 2.0210\n",
      "Epoch 7 Batch 400 Loss 2.0084\n",
      "Epoch 7 Loss 2.1660\n",
      "Time taken for 1 epoch 278.83004999160767 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.7012\n",
      "Epoch 8 Batch 100 Loss 2.0039\n",
      "Epoch 8 Batch 200 Loss 2.0127\n",
      "Epoch 8 Batch 300 Loss 1.9832\n",
      "Epoch 8 Batch 400 Loss 1.9832\n",
      "Epoch 8 Loss 2.1330\n",
      "Time taken for 1 epoch 278.96759939193726 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.6952\n",
      "Epoch 9 Batch 100 Loss 1.9833\n",
      "Epoch 9 Batch 200 Loss 1.9759\n",
      "Epoch 9 Batch 300 Loss 1.9507\n",
      "Epoch 9 Batch 400 Loss 1.9522\n",
      "Epoch 9 Loss 2.1045\n",
      "Time taken for 1 epoch 279.1003921031952 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.6886\n",
      "Epoch 10 Batch 100 Loss 1.9697\n",
      "Epoch 10 Batch 200 Loss 1.9456\n",
      "Epoch 10 Batch 300 Loss 1.9428\n",
      "Epoch 10 Batch 400 Loss 1.9256\n",
      "Epoch 10 Loss 2.0898\n",
      "Time taken for 1 epoch 279.41051483154297 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.6721\n",
      "Epoch 11 Batch 100 Loss 1.9357\n",
      "Epoch 11 Batch 200 Loss 1.9326\n",
      "Epoch 11 Batch 300 Loss 1.9262\n",
      "Epoch 11 Batch 400 Loss 1.8935\n",
      "Epoch 11 Loss 2.0615\n",
      "Time taken for 1 epoch 278.852778673172 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.6473\n",
      "Epoch 12 Batch 100 Loss 1.9172\n",
      "Epoch 12 Batch 200 Loss 1.9228\n",
      "Epoch 12 Batch 300 Loss 1.8863\n",
      "Epoch 12 Batch 400 Loss 1.8830\n",
      "Epoch 12 Loss 2.0294\n",
      "Time taken for 1 epoch 279.1161003112793 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.6481\n",
      "Epoch 13 Batch 100 Loss 1.8988\n",
      "Epoch 13 Batch 200 Loss 1.9071\n",
      "Epoch 13 Batch 300 Loss 1.8759\n",
      "Epoch 13 Batch 400 Loss 1.8625\n",
      "Epoch 13 Loss 2.0133\n",
      "Time taken for 1 epoch 278.3791353702545 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.6411\n",
      "Epoch 14 Batch 100 Loss 1.8874\n",
      "Epoch 14 Batch 200 Loss 1.9083\n",
      "Epoch 14 Batch 300 Loss 1.8474\n",
      "Epoch 14 Batch 400 Loss 1.8509\n",
      "Epoch 14 Loss 1.9957\n",
      "Time taken for 1 epoch 277.2961573600769 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.6325\n",
      "Epoch 15 Batch 100 Loss 1.8692\n",
      "Epoch 15 Batch 200 Loss 1.8660\n",
      "Epoch 15 Batch 300 Loss 1.8487\n",
      "Epoch 15 Batch 400 Loss 1.8474\n",
      "Epoch 15 Loss 1.9884\n",
      "Time taken for 1 epoch 277.289922952652 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.6305\n",
      "Epoch 16 Batch 100 Loss 1.8673\n",
      "Epoch 16 Batch 200 Loss 1.8542\n",
      "Epoch 16 Batch 300 Loss 1.8196\n",
      "Epoch 16 Batch 400 Loss 1.8171\n",
      "Epoch 16 Loss 1.9731\n",
      "Time taken for 1 epoch 277.30563712120056 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.6159\n",
      "Epoch 17 Batch 100 Loss 1.8356\n",
      "Epoch 17 Batch 200 Loss 1.8350\n",
      "Epoch 17 Batch 300 Loss 1.8076\n",
      "Epoch 17 Batch 400 Loss 1.8130\n",
      "Epoch 17 Loss 1.9294\n",
      "Time taken for 1 epoch 278.0057952404022 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.5987\n",
      "Epoch 18 Batch 100 Loss 1.8202\n",
      "Epoch 18 Batch 200 Loss 1.8358\n",
      "Epoch 18 Batch 300 Loss 1.7970\n",
      "Epoch 18 Batch 400 Loss 1.7959\n",
      "Epoch 18 Loss 1.9301\n",
      "Time taken for 1 epoch 277.40024399757385 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.6000\n",
      "Epoch 19 Batch 100 Loss 1.8111\n",
      "Epoch 19 Batch 200 Loss 1.8107\n",
      "Epoch 19 Batch 300 Loss 1.7953\n",
      "Epoch 19 Batch 400 Loss 1.7877\n",
      "Epoch 19 Loss 1.9178\n",
      "Time taken for 1 epoch 277.2093336582184 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.6027\n",
      "Epoch 20 Batch 100 Loss 1.7946\n",
      "Epoch 20 Batch 200 Loss 1.8116\n",
      "Epoch 20 Batch 300 Loss 1.7766\n",
      "Epoch 20 Batch 400 Loss 1.7763\n",
      "Epoch 20 Loss 1.9105\n",
      "Time taken for 1 epoch 277.1648669242859 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 1.5778\n",
      "Epoch 21 Batch 100 Loss 1.7892\n",
      "Epoch 21 Batch 200 Loss 1.8019\n",
      "Epoch 21 Batch 300 Loss 1.7646\n",
      "Epoch 21 Batch 400 Loss 1.7616\n",
      "Epoch 21 Loss 1.9226\n",
      "Time taken for 1 epoch 277.3599679470062 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 1.5729\n",
      "Epoch 22 Batch 100 Loss 1.7847\n",
      "Epoch 22 Batch 200 Loss 1.7844\n",
      "Epoch 22 Batch 300 Loss 1.7568\n",
      "Epoch 22 Batch 400 Loss 1.7587\n",
      "Epoch 22 Loss 1.8899\n",
      "Time taken for 1 epoch 277.00997138023376 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 1.5705\n",
      "Epoch 23 Batch 100 Loss 1.7719\n",
      "Epoch 23 Batch 200 Loss 1.7758\n",
      "Epoch 23 Batch 300 Loss 1.7371\n",
      "Epoch 23 Batch 400 Loss 1.7441\n",
      "Epoch 23 Loss 1.8859\n",
      "Time taken for 1 epoch 277.06364941596985 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 1.5661\n",
      "Epoch 24 Batch 100 Loss 1.7528\n",
      "Epoch 24 Batch 200 Loss 1.7688\n",
      "Epoch 24 Batch 300 Loss 1.7359\n",
      "Epoch 24 Batch 400 Loss 1.7291\n",
      "Epoch 24 Loss 1.8742\n",
      "Time taken for 1 epoch 277.1775760650635 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 1.5653\n",
      "Epoch 25 Batch 100 Loss 1.7504\n",
      "Epoch 25 Batch 200 Loss 1.7578\n",
      "Epoch 25 Batch 300 Loss 1.7281\n",
      "Epoch 25 Batch 400 Loss 1.7097\n",
      "Epoch 25 Loss 1.8713\n",
      "Time taken for 1 epoch 277.165864944458 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 1.5541\n",
      "Epoch 26 Batch 100 Loss 1.7530\n",
      "Epoch 26 Batch 200 Loss 1.7478\n",
      "Epoch 26 Batch 300 Loss 1.7052\n",
      "Epoch 26 Batch 400 Loss 1.7048\n",
      "Epoch 26 Loss 1.8361\n",
      "Time taken for 1 epoch 277.2116403579712 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 1.5424\n",
      "Epoch 27 Batch 100 Loss 1.7374\n",
      "Epoch 27 Batch 200 Loss 1.7320\n",
      "Epoch 27 Batch 300 Loss 1.7093\n",
      "Epoch 27 Batch 400 Loss 1.7000\n",
      "Epoch 27 Loss 1.8364\n",
      "Time taken for 1 epoch 277.39612650871277 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 1.5593\n",
      "Epoch 28 Batch 100 Loss 1.7188\n",
      "Epoch 28 Batch 200 Loss 1.7287\n",
      "Epoch 28 Batch 300 Loss 1.6975\n",
      "Epoch 28 Batch 400 Loss 1.7077\n",
      "Epoch 28 Loss 1.8208\n",
      "Time taken for 1 epoch 277.0512180328369 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 1.5444\n",
      "Epoch 29 Batch 100 Loss 1.7184\n",
      "Epoch 29 Batch 200 Loss 1.7045\n",
      "Epoch 29 Batch 300 Loss 1.7002\n",
      "Epoch 29 Batch 400 Loss 1.6902\n",
      "Epoch 29 Loss 1.8135\n",
      "Time taken for 1 epoch 277.0435426235199 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 1.5212\n",
      "Epoch 30 Batch 100 Loss 1.7163\n",
      "Epoch 30 Batch 200 Loss 1.7134\n",
      "Epoch 30 Batch 300 Loss 1.6838\n",
      "Epoch 30 Batch 400 Loss 1.6858\n",
      "Epoch 30 Loss 1.8129\n",
      "Time taken for 1 epoch 277.27601623535156 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 1.5171\n",
      "Epoch 31 Batch 100 Loss 1.7103\n",
      "Epoch 31 Batch 200 Loss 1.6889\n",
      "Epoch 31 Batch 300 Loss 1.6870\n",
      "Epoch 31 Batch 400 Loss 1.6734\n",
      "Epoch 31 Loss 1.8114\n",
      "Time taken for 1 epoch 277.1184895038605 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 1.5202\n",
      "Epoch 32 Batch 100 Loss 1.7020\n",
      "Epoch 32 Batch 200 Loss 1.7013\n",
      "Epoch 32 Batch 300 Loss 1.6763\n",
      "Epoch 32 Batch 400 Loss 1.6698\n",
      "Epoch 32 Loss 1.8021\n",
      "Time taken for 1 epoch 277.0398871898651 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 1.5107\n",
      "Epoch 33 Batch 100 Loss 1.7021\n",
      "Epoch 33 Batch 200 Loss 1.6893\n",
      "Epoch 33 Batch 300 Loss 1.6650\n",
      "Epoch 33 Batch 400 Loss 1.6396\n",
      "Epoch 33 Loss 1.7881\n",
      "Time taken for 1 epoch 277.08516216278076 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 1.4964\n",
      "Epoch 34 Batch 100 Loss 1.6887\n",
      "Epoch 34 Batch 200 Loss 1.6758\n",
      "Epoch 34 Batch 300 Loss 1.6552\n",
      "Epoch 34 Batch 400 Loss 1.6439\n",
      "Epoch 34 Loss 1.8036\n",
      "Time taken for 1 epoch 277.0408308506012 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 1.5094\n",
      "Epoch 35 Batch 100 Loss 1.6826\n",
      "Epoch 35 Batch 200 Loss 1.6641\n",
      "Epoch 35 Batch 300 Loss 1.6436\n",
      "Epoch 35 Batch 400 Loss 1.6429\n",
      "Epoch 35 Loss 1.7821\n",
      "Time taken for 1 epoch 277.5622363090515 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 1.5061\n",
      "Epoch 36 Batch 100 Loss 1.6697\n",
      "Epoch 36 Batch 200 Loss 1.6509\n",
      "Epoch 36 Batch 300 Loss 1.6544\n",
      "Epoch 36 Batch 400 Loss 1.6370\n",
      "Epoch 36 Loss 1.7732\n",
      "Time taken for 1 epoch 277.7768051624298 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 1.5005\n",
      "Epoch 37 Batch 100 Loss 1.6653\n",
      "Epoch 37 Batch 200 Loss 1.6612\n",
      "Epoch 37 Batch 300 Loss 1.6508\n",
      "Epoch 37 Batch 400 Loss 1.6280\n",
      "Epoch 37 Loss 1.7555\n",
      "Time taken for 1 epoch 277.6198945045471 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 1.4855\n",
      "Epoch 38 Batch 100 Loss 1.6510\n",
      "Epoch 38 Batch 200 Loss 1.6551\n",
      "Epoch 38 Batch 300 Loss 1.6420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Batch 400 Loss 1.6317\n",
      "Epoch 38 Loss 1.7529\n",
      "Time taken for 1 epoch 277.2389483451843 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 1.4815\n",
      "Epoch 39 Batch 100 Loss 1.6512\n",
      "Epoch 39 Batch 200 Loss 1.6501\n",
      "Epoch 39 Batch 300 Loss 1.6182\n",
      "Epoch 39 Batch 400 Loss 1.6125\n",
      "Epoch 39 Loss 1.7463\n",
      "Time taken for 1 epoch 277.439297914505 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 1.4707\n",
      "Epoch 40 Batch 100 Loss 1.6529\n",
      "Epoch 40 Batch 200 Loss 1.6342\n",
      "Epoch 40 Batch 300 Loss 1.6309\n",
      "Epoch 40 Batch 400 Loss 1.6034\n",
      "Epoch 40 Loss 1.7346\n",
      "Time taken for 1 epoch 277.63431692123413 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 1.4707\n",
      "Epoch 41 Batch 100 Loss 1.6345\n",
      "Epoch 41 Batch 200 Loss 1.6251\n",
      "Epoch 41 Batch 300 Loss 1.6307\n",
      "Epoch 41 Batch 400 Loss 1.6047\n",
      "Epoch 41 Loss 1.7267\n",
      "Time taken for 1 epoch 277.1708221435547 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 1.4623\n",
      "Epoch 42 Batch 100 Loss 1.6271\n",
      "Epoch 42 Batch 200 Loss 1.6181\n",
      "Epoch 42 Batch 300 Loss 1.6349\n",
      "Epoch 42 Batch 400 Loss 1.5950\n",
      "Epoch 42 Loss 1.7220\n",
      "Time taken for 1 epoch 277.0613114833832 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 1.4695\n",
      "Epoch 43 Batch 100 Loss 1.6198\n",
      "Epoch 43 Batch 200 Loss 1.6319\n",
      "Epoch 43 Batch 300 Loss 1.6113\n",
      "Epoch 43 Batch 400 Loss 1.5962\n",
      "Epoch 43 Loss 1.7270\n",
      "Time taken for 1 epoch 277.05417251586914 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 1.4651\n",
      "Epoch 44 Batch 100 Loss 1.6170\n",
      "Epoch 44 Batch 200 Loss 1.5987\n",
      "Epoch 44 Batch 300 Loss 1.6040\n",
      "Epoch 44 Batch 400 Loss 1.6043\n",
      "Epoch 44 Loss 1.7078\n",
      "Time taken for 1 epoch 276.997594833374 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 1.4563\n",
      "Epoch 45 Batch 100 Loss 1.6011\n",
      "Epoch 45 Batch 200 Loss 1.6107\n",
      "Epoch 45 Batch 300 Loss 1.6133\n",
      "Epoch 45 Batch 400 Loss 1.5716\n",
      "Epoch 45 Loss 1.7114\n",
      "Time taken for 1 epoch 277.2902455329895 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 1.4577\n",
      "Epoch 46 Batch 100 Loss 1.5995\n",
      "Epoch 46 Batch 200 Loss 1.5972\n",
      "Epoch 46 Batch 300 Loss 1.5941\n",
      "Epoch 46 Batch 400 Loss 1.5577\n",
      "Epoch 46 Loss 1.6932\n",
      "Time taken for 1 epoch 277.2397677898407 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 1.4476\n",
      "Epoch 47 Batch 100 Loss 1.5991\n",
      "Epoch 47 Batch 200 Loss 1.6027\n",
      "Epoch 47 Batch 300 Loss 1.5772\n",
      "Epoch 47 Batch 400 Loss 1.5637\n",
      "Epoch 47 Loss 1.6947\n",
      "Time taken for 1 epoch 277.35990715026855 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 1.4410\n",
      "Epoch 48 Batch 100 Loss 1.6050\n",
      "Epoch 48 Batch 200 Loss 1.5941\n",
      "Epoch 48 Batch 300 Loss 1.5907\n",
      "Epoch 48 Batch 400 Loss 1.5589\n",
      "Epoch 48 Loss 1.6844\n",
      "Time taken for 1 epoch 277.27842378616333 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 1.4524\n",
      "Epoch 49 Batch 100 Loss 1.5970\n",
      "Epoch 49 Batch 200 Loss 1.5737\n",
      "Epoch 49 Batch 300 Loss 1.5647\n",
      "Epoch 49 Batch 400 Loss 1.5675\n",
      "Epoch 49 Loss 1.6823\n",
      "Time taken for 1 epoch 277.31515622138977 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 1.4507\n",
      "Epoch 50 Batch 100 Loss 1.5906\n",
      "Epoch 50 Batch 200 Loss 1.5964\n",
      "Epoch 50 Batch 300 Loss 1.5700\n",
      "Epoch 50 Batch 400 Loss 1.5718\n",
      "Epoch 50 Loss 1.6742\n",
      "Time taken for 1 epoch 277.30091977119446 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = './cnm_training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "model = Model(vocab_size, embedding_dim, units)\n",
    "model.load_weights(checkpoint_prefix)\n",
    "model.build(tf.TensorShape([BATCH_SIZE, seq_length]))\n",
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    hidden = model.reset_states()\n",
    "    for (batch, (inp, target)) in enumerate(dataset):\n",
    "          with tf.GradientTape() as tape:\n",
    "              predictions = model(inp)\n",
    "              loss = loss_function(target, predictions)\n",
    "          grads = tape.gradient(loss, model.variables)\n",
    "          optimizer.apply_gradients(zip(grads, model.variables))\n",
    "          if batch % 100 == 0:\n",
    "              print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,batch,loss))\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "      model.save_weights(checkpoint_prefix)\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "model.save_weights(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为金色，单是这种都是十分熟悉的团长。一旦被他顶得杀神领域地地方，魔魂大白鲨都吃下了亢奋粉红肠，吞噬后打动了。而且现在只能从唐三释飞到六十级层层甲上破碎。当时，他心中却蒙着了一层阴清，一动身形的踏鬼影迷踪步而去。如果在前一世的他再将其阻挡他也发挥不出任何作用。单是唐三手中海神三叉戟上释放的血焰对唐三来说还是极为少见的。但他可以肯定，有不少魂师实力的。\n",
      "一边说着，大海中被金色渲染的唐三眼底深不过低。看到它。”海德尔前方。喊出了一个询问，从上面凝聚着一层紫波瞬间爆发出先前那一击。瞬间破开泥土。将两株大树随之破碎而入。如同封号斗罗地实力有多么恐怖，就要将他完全笼罩在内。到整个身体脱层皮肤。\n",
      "看到魂师胸前，那张看着奥斯卡道：“小三，你的第四考是什么？我等大量的消耗\n",
      "别地意义是什么？”\n",
      "奥斯卡如此声音似的，白沉香却久了不着两人，被唐三拦在晕船，海神三叉戟从最前面的左眼中吐出，只剩下由衷的巨大地草。这种方法也很容易感觉到她那强悍的能量波动。\n",
      "暗魔邪神虎眼中已是焰光芒反而迎向了血脉，封死了海龙斗罗的时机。\n",
      "杀戮之王的厉电爆发。属于唐三的攻击本体不只是因为他们的攻击力而惊慌。可实际上，他的龟甲盾也能释放到十万年级别。\n",
      "这一次她想要释放比诸葛神弩的威力，首先就看不到她怎么办？怎么也说不清。但现在自己却流露着希望能够真正的将自己地所能改变自己的经脉，至少可能让小舞留在那里后。\n",
      "玄天功内力涌入对方群体的行动，五万年级别地魂兽，哪怕是当年魂师或者大斗魂场外观的大多数都是以铁甲为为传。因为他们眼中的景象已经因此而沉。下错，当地唐三也不所能留手。但是，唐三可是准备第二次与你决定。他刚刚到武魂帝国的情况下。这次我们攻击是迅雷直接攻击降临，这样的强者会极少威胁。你们听到海星七圣柱的剑斗罗，到礼数正上方，请回家三平房。\n",
      "仅仅是，这次并不多厅，众人再也忍不住笑出来了。三个时辰过去了。缓缓抬头，看向唐三，“如果我不能活着走过去，那么，我就告诉你。我是爱嘴上拼，哪怕是遇到危险的时候也难以无与了死。恶狠的对手也是空强的情况。在我心中。实早一定智的与唐三等人。在下一刻选定要说。谁能达到最终威力的。将自己最后地魂师组合以及时昂的消失，整个弩机仿佛都在这股能量压制下的情况被荡起。尽管大部分魂师都突破了九级。就在这突如象来看那里将群起来。\n",
      "独孤博道：“现在我在一起的时候，已经成就了海神，如果回答不会手，那如此虚空，就扫\n"
     ]
    }
   ],
   "source": [
    "model = Model(vocab_size, embedding_dim, units)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "num_generate = 1000\n",
    "start_string = '为'\n",
    "input_eval = [char2idx[s] for s in start_string]\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "text_generated = []\n",
    "temperature = 1.0\n",
    "model.reset_states()\n",
    "for i in range(num_generate):\n",
    "    predictions = model(input_eval)\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "    predictions = predictions / temperature\n",
    "    predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    text_generated.append(idx2char[predicted_id])\n",
    "print (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
